#!/usr/bin/env python3
"""
å®Œæ•´è®­ç»ƒè„šæœ¬
æ”¯æŒ BNaT æ•°æ®é›†çš„å®Œæ•´è®­ç»ƒï¼ŒåŒ…æ‹¬ Merged å’Œ DL æ•°æ®é›†
"""

import os
import sys
import logging
import time
import pandas as pd
import numpy as np
from datetime import datetime
import json
import glob
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def find_bnat_datasets():
    """æŸ¥æ‰¾ BNaT æ•°æ®é›†æ–‡ä»¶"""
    print("ğŸ” æŸ¥æ‰¾ BNaT æ•°æ®é›†...")
    
    # å¯èƒ½çš„è·¯å¾„æ¨¡å¼
    base_paths = [
        '.',
        'BlockFW',
        'BlockFW/BlockFW',
        '/content/BlockFW',
        '/content/BlockFW/BlockFW'
    ]
    
    dataset_patterns = [
        'ml/data/BNaT-master/Merged_dataset/*.csv',
        'ml/data/BNaT-master/DL_dataset/*.csv',
        'data/BNaT-master/Merged_dataset/*.csv',
        'data/BNaT-master/DL_dataset/*.csv',
        'BNaT-master/Merged_dataset/*.csv',
        'BNaT-master/DL_dataset/*.csv'
    ]
    
    found_datasets = {}
    
    for base_path in base_paths:
        if not os.path.exists(base_path):
            continue
            
        print(f"æ£€æŸ¥è·¯å¾„: {base_path}")
        
        for pattern in dataset_patterns:
            full_pattern = os.path.join(base_path, pattern)
            files = glob.glob(full_pattern)
            
            if files:
                dataset_type = 'Merged' if 'Merged' in pattern else 'DL'
                if dataset_type not in found_datasets:
                    found_datasets[dataset_type] = []
                found_datasets[dataset_type].extend(files)
                print(f"  âœ“ æ‰¾åˆ° {dataset_type} æ•°æ®é›†: {len(files)} ä¸ªæ–‡ä»¶")
                for file in files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
                    print(f"    - {file}")
                if len(files) > 3:
                    print(f"    ... è¿˜æœ‰ {len(files) - 3} ä¸ªæ–‡ä»¶")
    
    return found_datasets

def load_dataset_files(file_paths, dataset_type):
    """åŠ è½½æ•°æ®é›†æ–‡ä»¶"""
    print(f"\nğŸ“Š åŠ è½½ {dataset_type} æ•°æ®é›†...")
    
    all_data = []
    total_rows = 0
    
    for file_path in file_paths:
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
            print(f"åŠ è½½æ–‡ä»¶: {file_path} ({file_size:.1f} MB)")
            
            # è¯»å–æ•°æ®
            data = pd.read_csv(file_path, header=None)
            
            # æ£€æŸ¥åˆ—æ•°
            if len(data.columns) == 22:  # 21ä¸ªç‰¹å¾ + 1ä¸ªæ ‡ç­¾
                feature_cols = [f'feature_{i}' for i in range(21)]
                data.columns = feature_cols + ['label']
            elif len(data.columns) == 21:  # åªæœ‰ç‰¹å¾ï¼Œæ²¡æœ‰æ ‡ç­¾
                feature_cols = [f'feature_{i}' for i in range(21)]
                data.columns = feature_cols
                # ä¸º DL æ•°æ®é›†æ·»åŠ æ ‡ç­¾ï¼ˆå‡è®¾éƒ½æ˜¯æ­£å¸¸æµé‡ï¼‰
                data['label'] = 'Normal'
            else:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {file_path}: åˆ—æ•°ä¸ç¬¦åˆé¢„æœŸ ({len(data.columns)})")
                continue
            
            all_data.append(data)
            total_rows += len(data)
            print(f"  âœ“ åŠ è½½æˆåŠŸ: {len(data)} è¡Œ")
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
    
    if not all_data:
        print(f"âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½• {dataset_type} æ•°æ®")
        return None
    
    # åˆå¹¶æ•°æ®
    merged_data = pd.concat(all_data, ignore_index=True)
    print(f"âœ“ {dataset_type} æ•°æ®é›†åˆå¹¶å®Œæˆ: {merged_data.shape} (æ€»è¡Œæ•°: {total_rows})")
    
    return merged_data

def preprocess_data(data, dataset_name):
    """æ•°æ®é¢„å¤„ç†"""
    print(f"\nğŸ”§ é¢„å¤„ç† {dataset_name} æ•°æ®...")
    
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    X = data.drop('label', axis=1)
    y = data['label']
    
    print(f"ç‰¹å¾æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"æ ‡ç­¾æ•°æ®å½¢çŠ¶: {y.shape}")
    
    # æ£€æŸ¥æ•°æ®ç±»å‹
    categorical_features = []
    numerical_features = []
    
    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'string':
            categorical_features.append(col)
        else:
            numerical_features.append(col)
    
    print(f"å‘ç° {len(categorical_features)} ä¸ªåˆ†ç±»ç‰¹å¾ï¼Œ{len(numerical_features)} ä¸ªæ•°å€¼ç‰¹å¾")
    
    # å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œç¼–ç 
    if categorical_features:
        print("ç¼–ç åˆ†ç±»ç‰¹å¾...")
        for col in categorical_features:
            unique_values = X[col].unique()
            print(f"  {col}: {unique_values[:5]} -> ç¼–ç å®Œæˆ")
            X[col] = pd.Categorical(X[col]).codes
    
    # å¤„ç†ç¼ºå¤±å€¼
    missing_values = X.isnull().sum()
    if missing_values.sum() > 0:
        print(f"å‘ç°ç¼ºå¤±å€¼: {missing_values[missing_values > 0]}")
        X = X.fillna(X.median())
        print("å·²ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼")
    else:
        print("âœ“ æ²¡æœ‰ç¼ºå¤±å€¼")
    
    # ç¼–ç æ ‡ç­¾
    print("ç¼–ç æ ‡ç­¾...")
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    print(f"æ ‡ç­¾ç±»åˆ«: {list(le.classes_)}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_encoded)}")
    
    # æ ‡å‡†åŒ–
    print("æ ‡å‡†åŒ–ç‰¹å¾...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    print(f"âœ“ æ ‡å‡†åŒ–å®Œæˆï¼Œç‰¹å¾å½¢çŠ¶: {X_scaled.shape}")
    
    return X_scaled, y_encoded, scaler, le

def train_models(X_scaled, y_encoded, dataset_name):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"\nğŸ¤– è®­ç»ƒ {dataset_name} æ¨¡å‹...")
    
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score, classification_report
    import time
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"è®­ç»ƒé›†: {X_train.shape}")
    print(f"æµ‹è¯•é›†: {X_test.shape}")
    
    results = {}
    
    # 1. éšæœºæ£®æ—
    print("\nè®­ç»ƒéšæœºæ£®æ—...")
    start_time = time.time()
    rf = RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    rf.fit(X_train, y_train)
    rf_time = time.time() - start_time
    
    y_pred_rf = rf.predict(X_test)
    rf_accuracy = accuracy_score(y_test, y_pred_rf)
    
    results['random_forest'] = {
        'model': rf,
        'time': rf_time,
        'accuracy': rf_accuracy,
        'gpu': False,
        'predictions': y_pred_rf,
        'true_labels': y_test
    }
    
    print(f"éšæœºæ£®æ—: {rf_time:.2f}ç§’, å‡†ç¡®ç‡: {rf_accuracy:.4f}")
    
    # 2. XGBoostï¼ˆGPUï¼‰
    try:
        import xgboost as xgb
        print("\nè®­ç»ƒ XGBoost (GPU)...")
        
        start_time = time.time()
        xgb_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.1,
            tree_method='hist',
            device='cuda',
            random_state=42
        )
        xgb_model.fit(X_train, y_train)
        xgb_time = time.time() - start_time
        
        y_pred_xgb = xgb_model.predict(X_test)
        xgb_accuracy = accuracy_score(y_test, y_pred_xgb)
        
        results['xgboost'] = {
            'model': xgb_model,
            'time': xgb_time,
            'accuracy': xgb_accuracy,
            'gpu': True,
            'predictions': y_pred_xgb,
            'true_labels': y_test
        }
        
        print(f"XGBoost: {xgb_time:.2f}ç§’, å‡†ç¡®ç‡: {xgb_accuracy:.4f}")
        
    except Exception as e:
        print(f"âš ï¸ XGBoost è®­ç»ƒå¤±è´¥: {e}")
    
    # 3. PyTorch ç¥ç»ç½‘ç»œï¼ˆGPUï¼‰
    try:
        import torch
        import torch.nn as nn
        import torch.optim as optim
        
        if torch.cuda.is_available():
            print("\nè®­ç»ƒ PyTorch ç¥ç»ç½‘ç»œ (GPU)...")
            
            class FullNN(nn.Module):
                def __init__(self, input_size, hidden_size, output_size):
                    super(FullNN, self).__init__()
                    self.fc1 = nn.Linear(input_size, hidden_size)
                    self.bn1 = nn.BatchNorm1d(hidden_size)
                    self.relu = nn.ReLU()
                    self.dropout = nn.Dropout(0.3)
                    self.fc2 = nn.Linear(hidden_size, hidden_size // 2)
                    self.bn2 = nn.BatchNorm1d(hidden_size // 2)
                    self.fc3 = nn.Linear(hidden_size // 2, hidden_size // 4)
                    self.bn3 = nn.BatchNorm1d(hidden_size // 4)
                    self.fc4 = nn.Linear(hidden_size // 4, output_size)
                    
                def forward(self, x):
                    x = self.fc1(x)
                    x = self.bn1(x)
                    x = self.relu(x)
                    x = self.dropout(x)
                    x = self.fc2(x)
                    x = self.bn2(x)
                    x = self.relu(x)
                    x = self.dropout(x)
                    x = self.fc3(x)
                    x = self.bn3(x)
                    x = self.relu(x)
                    x = self.dropout(x)
                    x = self.fc4(x)
                    return x
            
            # è½¬æ¢ä¸ºå¼ é‡
            X_train_tensor = torch.FloatTensor(X_train).cuda()
            y_train_tensor = torch.LongTensor(y_train).cuda()
            X_test_tensor = torch.FloatTensor(X_test).cuda()
            y_test_tensor = torch.LongTensor(y_test).cuda()
            
            # åˆ›å»ºæ¨¡å‹
            input_size = X_train.shape[1]
            output_size = len(np.unique(y_encoded))
            model = FullNN(input_size, 256, output_size).cuda()
            
            # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
            criterion = nn.CrossEntropyLoss()
            
            # è®­ç»ƒ
            start_time = time.time()
            model.train()
            for epoch in range(100):
                optimizer.zero_grad()
                outputs = model(X_train_tensor)
                loss = criterion(outputs, y_train_tensor)
                loss.backward()
                optimizer.step()
                
                if (epoch + 1) % 20 == 0:
                    print(f"  Epoch {epoch+1}/100, Loss: {loss.item():.4f}")
            
            torch.cuda.synchronize()
            nn_time = time.time() - start_time
            
            # è¯„ä¼°
            model.eval()
            with torch.no_grad():
                outputs = model(X_test_tensor)
                _, predicted = torch.max(outputs, 1)
                nn_accuracy = (predicted == y_test_tensor).float().mean().item()
            
            results['pytorch_nn'] = {
                'model': model,
                'time': nn_time,
                'accuracy': nn_accuracy,
                'gpu': True,
                'predictions': predicted.cpu().numpy(),
                'true_labels': y_test
            }
            
            print(f"PyTorch NN: {nn_time:.2f}ç§’, å‡†ç¡®ç‡: {nn_accuracy:.4f}")
            
        else:
            print("âš ï¸ GPU ä¸å¯ç”¨ï¼Œè·³è¿‡ PyTorch è®­ç»ƒ")
            
    except Exception as e:
        print(f"âš ï¸ PyTorch è®­ç»ƒå¤±è´¥: {e}")
    
    return results

def save_results(results, scaler, label_encoder, dataset_name):
    """ä¿å­˜ç»“æœ"""
    print(f"\nğŸ’¾ ä¿å­˜ {dataset_name} ç»“æœ...")
    
    # åˆ›å»ºæ¨¡å‹ç›®å½•
    os.makedirs('BlockFW/ml/models', exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        'dataset_name': dataset_name,
        'training_time': datetime.now().isoformat(),
        'data_info': {
            'total_samples': len(label_encoder.classes_),
            'classes': list(label_encoder.classes_),
            'class_distribution': np.bincount(label_encoder.transform(label_encoder.classes_)).tolist()
        },
        'results': {
            name: {
                'time': result['time'],
                'accuracy': result['accuracy'],
                'gpu': result['gpu']
            } for name, result in results.items()
        },
        'best_model': max(results.keys(), key=lambda k: results[k]['accuracy']),
        'best_accuracy': max(result['accuracy'] for result in results.values())
    }
    
    report_path = f'BlockFW/ml/models/full_{dataset_name}_report_{timestamp}.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # å°è¯•ä¿å­˜æ¨¡å‹
    try:
        import joblib
        import torch
        
        for model_name, result in results.items():
            if model_name == 'pytorch_nn':
                model_path = f'BlockFW/ml/models/full_{dataset_name}_{model_name}_{timestamp}.pth'
                torch.save(result['model'].state_dict(), model_path)
            else:
                model_path = f'BlockFW/ml/models/full_{dataset_name}_{model_name}_{timestamp}.pkl'
                joblib.dump(result['model'], model_path)
            
            print(f"ä¿å­˜ {model_name}: {model_path}")
        
        # ä¿å­˜é¢„å¤„ç†å™¨
        scaler_path = f'BlockFW/ml/models/full_{dataset_name}_scaler_{timestamp}.pkl'
        joblib.dump(scaler, scaler_path)
        
        encoder_path = f'BlockFW/ml/models/full_{dataset_name}_encoder_{timestamp}.pkl'
        joblib.dump(label_encoder, encoder_path)
        
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        print("ä½†æŠ¥å‘Šå·²æˆåŠŸä¿å­˜")
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ BNaT å®Œæ•´è®­ç»ƒ")
    print("="*60)
    
    # 1. æŸ¥æ‰¾æ•°æ®é›†
    datasets = find_bnat_datasets()
    
    if not datasets:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• BNaT æ•°æ®é›†")
        print("\nè¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„ï¼Œç¡®ä¿åŒ…å«ä»¥ä¸‹æ–‡ä»¶ä¹‹ä¸€:")
        print("- ml/data/BNaT-master/Merged_dataset/*.csv")
        print("- ml/data/BNaT-master/DL_dataset/*.csv")
        return
    
    all_results = {}
    
    # 2. å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for dataset_type, file_paths in datasets.items():
        print(f"\n{'='*60}")
        print(f"å¤„ç† {dataset_type} æ•°æ®é›†")
        print(f"{'='*60}")
        
        # åŠ è½½æ•°æ®
        data = load_dataset_files(file_paths, dataset_type)
        if data is None:
            continue
        
        # é¢„å¤„ç†æ•°æ®
        X_scaled, y_encoded, scaler, le = preprocess_data(data, dataset_type)
        
        # è®­ç»ƒæ¨¡å‹
        results = train_models(X_scaled, y_encoded, dataset_type)
        
        if not results:
            print(f"âŒ {dataset_type} æ•°æ®é›†è®­ç»ƒå¤±è´¥")
            continue
        
        # ä¿å­˜ç»“æœ
        report = save_results(results, scaler, le, dataset_type)
        all_results[dataset_type] = report
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š {dataset_type} è®­ç»ƒç»“æœ:")
        for name, result in results.items():
            gpu_status = "GPU" if result['gpu'] else "CPU"
            print(f"  {name} ({gpu_status}): {result['time']:.2f}ç§’, å‡†ç¡®ç‡: {result['accuracy']:.4f}")
        
        print(f"æœ€ä½³æ¨¡å‹: {report['best_model']}")
        print(f"æœ€ä½³å‡†ç¡®ç‡: {report['best_accuracy']:.4f}")
    
    # 3. æ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ‰ å®Œæ•´è®­ç»ƒæ€»ç»“")
    print(f"{'='*60}")
    
    for dataset_type, report in all_results.items():
        print(f"\n{dataset_type} æ•°æ®é›†:")
        print(f"  æœ€ä½³æ¨¡å‹: {report['best_model']}")
        print(f"  æœ€ä½³å‡†ç¡®ç‡: {report['best_accuracy']:.4f}")
        print(f"  è®­ç»ƒæ—¶é—´: {report['training_time']}")
    
    print(f"\næ‰€æœ‰æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜åˆ° BlockFW/ml/models/ ç›®å½•")

if __name__ == "__main__":
    main() 