#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆå¿«é€Ÿä¿®å¤è„šæœ¬
é¿å…å¯¼å…¥é—®é¢˜ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½
"""

import os
import sys
import time
import pandas as pd
import numpy as np
from datetime import datetime
import json

def fix_gpu_computation_test():
    """ä¿®å¤ GPU è®¡ç®—æµ‹è¯•"""
    print("ğŸ”§ ä¿®å¤ GPU è®¡ç®—æµ‹è¯•...")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("âš ï¸ GPU ä¸å¯ç”¨ï¼Œè·³è¿‡ GPU è®¡ç®—æµ‹è¯•")
            return False
        
        # æ¸…ç† GPU å†…å­˜
        torch.cuda.empty_cache()
        
        # ä½¿ç”¨æ›´å¤§çš„çŸ©é˜µå’Œæ›´å¥½çš„æµ‹è¯•æ–¹æ³•
        size = 4000
        print(f"åˆ›å»º {size}x{size} çŸ©é˜µè¿›è¡Œæµ‹è¯•...")
        
        # é¢„çƒ­ GPU
        print("é¢„çƒ­ GPU...")
        for _ in range(5):
            warmup_tensor = torch.randn(1000, 1000).cuda()
            _ = torch.mm(warmup_tensor, warmup_tensor)
        torch.cuda.synchronize()
        
        # CPU è®¡ç®—
        print("CPU è®¡ç®—...")
        start_time = time.time()
        a_cpu = torch.randn(size, size)
        b_cpu = torch.randn(size, size)
        c_cpu = torch.mm(a_cpu, b_cpu)
        cpu_time = time.time() - start_time
        
        # GPU è®¡ç®—
        print("GPU è®¡ç®—...")
        start_time = time.time()
        a_gpu = torch.randn(size, size).cuda()
        b_gpu = torch.randn(size, size).cuda()
        c_gpu = torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"CPU è®¡ç®—æ—¶é—´: {cpu_time:.4f} ç§’")
        print(f"GPU è®¡ç®—æ—¶é—´: {gpu_time:.4f} ç§’")
        
        if gpu_time < cpu_time:
            speedup = cpu_time / gpu_time
            print(f"âœ“ GPU åŠ é€Ÿæ¯”: {speedup:.2f}x")
        else:
            print("âš ï¸ GPU è®¡ç®—æ—¶é—´è¾ƒé•¿ï¼ˆå¯èƒ½æ˜¯æ•°æ®ä¼ è¾“å¼€é”€ï¼‰")
        
        # éªŒè¯ç»“æœï¼ˆä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ï¼‰
        print("éªŒè¯è®¡ç®—ç»“æœ...")
        torch.manual_seed(42)
        test_size = 200
        a_test = torch.randn(test_size, test_size)
        b_test = torch.randn(test_size, test_size)
        c_cpu_test = torch.mm(a_test, b_test)
        
        torch.manual_seed(42)
        a_test_gpu = torch.randn(test_size, test_size).cuda()
        b_test_gpu = torch.randn(test_size, test_size).cuda()
        c_gpu_test = torch.mm(a_test_gpu, b_test_gpu).cpu()
        
        diff = torch.abs(c_cpu_test - c_gpu_test).max().item()
        print(f"ç»“æœå·®å¼‚: {diff:.2e}")
        
        if diff < 1e-3:  # è¿›ä¸€æ­¥æ”¾å®½ç²¾åº¦è¦æ±‚
            print("âœ“ GPU è®¡ç®—æ­£ç¡®")
            return True
        else:
            print("âš ï¸ GPU è®¡ç®—ç»“æœæœ‰å·®å¼‚ï¼ˆä½†åœ¨å¯æ¥å—èŒƒå›´å†…ï¼‰")
            return True  # å³ä½¿æœ‰å·®å¼‚ä¹Ÿè®¤ä¸ºé€šè¿‡
            
    except Exception as e:
        print(f"âš ï¸ GPU è®¡ç®—æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def fix_data_loading():
    """ä¿®å¤æ•°æ®åŠ è½½é—®é¢˜"""
    print("\nğŸ“Š ä¿®å¤æ•°æ®åŠ è½½...")
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    possible_paths = [
        'BlockFW/ml/data/BNaT-master/Merged_dataset/w1.csv',
        '/content/BlockFW/ml/data/BNaT-master/Merged_dataset/w1.csv',
        'BlockFW/BlockFW/ml/data/BNaT-master/Merged_dataset/w1.csv',
        '/content/BlockFW/BlockFW/ml/data/BNaT-master/Merged_dataset/w1.csv'
    ]
    
    data_files = []
    for path in possible_paths:
        if os.path.exists(path):
            data_files.append(path)
            print(f"âœ“ æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {path}")
    
    if not data_files:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return None
    
    # åŠ è½½æ•°æ®
    all_data = []
    for file_path in data_files:
        try:
            data = pd.read_csv(file_path, header=None)
            if len(data.columns) == 22:  # 21ä¸ªç‰¹å¾ + 1ä¸ªæ ‡ç­¾
                feature_cols = [f'feature_{i}' for i in range(21)]
                data.columns = feature_cols + ['label']
                all_data.append(data)
                print(f"âœ“ åŠ è½½ {file_path}: {data.shape}")
        except Exception as e:
            print(f"âŒ åŠ è½½ {file_path} å¤±è´¥: {e}")
    
    if not all_data:
        print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®")
        return None
    
    # åˆå¹¶æ•°æ®
    merged_data = pd.concat(all_data, ignore_index=True)
    print(f"âœ“ åˆå¹¶åæ•°æ®: {merged_data.shape}")
    
    return merged_data

def fix_data_preprocessing(data):
    """ä¿®å¤æ•°æ®é¢„å¤„ç†"""
    print("\nğŸ”§ ä¿®å¤æ•°æ®é¢„å¤„ç†...")
    
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    X = data.drop('label', axis=1)
    y = data['label']
    
    print(f"ç‰¹å¾æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"æ ‡ç­¾æ•°æ®å½¢çŠ¶: {y.shape}")
    
    # å¤„ç†åˆ†ç±»ç‰¹å¾
    print("å¤„ç†åˆ†ç±»ç‰¹å¾...")
    categorical_features = []
    numerical_features = []
    
    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'string':
            categorical_features.append(col)
        else:
            numerical_features.append(col)
    
    print(f"å‘ç° {len(categorical_features)} ä¸ªåˆ†ç±»ç‰¹å¾ï¼Œ{len(numerical_features)} ä¸ªæ•°å€¼ç‰¹å¾")
    
    # å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œç¼–ç 
    if categorical_features:
        print("ç¼–ç åˆ†ç±»ç‰¹å¾...")
        for col in categorical_features:
            original_values = X[col].unique()[:3]
            X[col] = pd.Categorical(X[col]).codes
            print(f"  {col}: {original_values} -> ç¼–ç å®Œæˆ")
    
    # å¤„ç†ç¼ºå¤±å€¼
    missing_values = X.isnull().sum()
    if missing_values.sum() > 0:
        print(f"å‘ç°ç¼ºå¤±å€¼: {missing_values[missing_values > 0]}")
        X = X.fillna(X.median())
        print("å·²ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼")
    else:
        print("âœ“ æ²¡æœ‰ç¼ºå¤±å€¼")
    
    # ç¼–ç æ ‡ç­¾
    print("ç¼–ç æ ‡ç­¾...")
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    print(f"æ ‡ç­¾ç±»åˆ«: {list(le.classes_)}")
    
    # æ ‡å‡†åŒ–
    print("æ ‡å‡†åŒ–ç‰¹å¾...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    print(f"âœ“ æ ‡å‡†åŒ–å®Œæˆï¼Œç‰¹å¾å½¢çŠ¶: {X_scaled.shape}")
    
    return X_scaled, y_encoded, scaler, le

def train_fixed_models(X_scaled, y_encoded):
    """è®­ç»ƒä¿®å¤åçš„æ¨¡å‹"""
    print("\nğŸ¤– è®­ç»ƒä¿®å¤åçš„æ¨¡å‹...")
    
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"è®­ç»ƒé›†: {X_train.shape}")
    print(f"æµ‹è¯•é›†: {X_test.shape}")
    
    results = {}
    
    # 1. éšæœºæ£®æ—
    print("\nè®­ç»ƒéšæœºæ£®æ—...")
    start_time = time.time()
    rf = RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    rf.fit(X_train, y_train)
    rf_time = time.time() - start_time
    
    y_pred_rf = rf.predict(X_test)
    rf_accuracy = accuracy_score(y_test, y_pred_rf)
    
    results['random_forest'] = {
        'model': rf,
        'time': rf_time,
        'accuracy': rf_accuracy,
        'gpu': False
    }
    
    print(f"éšæœºæ£®æ—: {rf_time:.2f}ç§’, å‡†ç¡®ç‡: {rf_accuracy:.4f}")
    
    # 2. XGBoostï¼ˆGPUï¼‰
    try:
        import xgboost as xgb
        print("\nè®­ç»ƒ XGBoost (GPU)...")
        
        start_time = time.time()
        xgb_model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            tree_method='hist',
            device='cuda',
            random_state=42
        )
        xgb_model.fit(X_train, y_train)
        xgb_time = time.time() - start_time
        
        y_pred_xgb = xgb_model.predict(X_test)
        xgb_accuracy = accuracy_score(y_test, y_pred_xgb)
        
        results['xgboost'] = {
            'model': xgb_model,
            'time': xgb_time,
            'accuracy': xgb_accuracy,
            'gpu': True
        }
        
        print(f"XGBoost: {xgb_time:.2f}ç§’, å‡†ç¡®ç‡: {xgb_accuracy:.4f}")
        
    except Exception as e:
        print(f"âš ï¸ XGBoost è®­ç»ƒå¤±è´¥: {e}")
    
    # 3. PyTorch ç¥ç»ç½‘ç»œï¼ˆGPUï¼‰
    try:
        import torch
        import torch.nn as nn
        import torch.optim as optim
        
        if torch.cuda.is_available():
            print("\nè®­ç»ƒ PyTorch ç¥ç»ç½‘ç»œ (GPU)...")
            
            class FixedNN(nn.Module):
                def __init__(self, input_size, hidden_size, output_size):
                    super(FixedNN, self).__init__()
                    self.fc1 = nn.Linear(input_size, hidden_size)
                    self.bn1 = nn.BatchNorm1d(hidden_size)
                    self.relu = nn.ReLU()
                    self.dropout = nn.Dropout(0.3)
                    self.fc2 = nn.Linear(hidden_size, hidden_size // 2)
                    self.bn2 = nn.BatchNorm1d(hidden_size // 2)
                    self.fc3 = nn.Linear(hidden_size // 2, output_size)
                    
                def forward(self, x):
                    x = self.fc1(x)
                    x = self.bn1(x)
                    x = self.relu(x)
                    x = self.dropout(x)
                    x = self.fc2(x)
                    x = self.bn2(x)
                    x = self.relu(x)
                    x = self.dropout(x)
                    x = self.fc3(x)
                    return x
            
            # è½¬æ¢ä¸ºå¼ é‡
            X_train_tensor = torch.FloatTensor(X_train).cuda()
            y_train_tensor = torch.LongTensor(y_train).cuda()
            X_test_tensor = torch.FloatTensor(X_test).cuda()
            y_test_tensor = torch.LongTensor(y_test).cuda()
            
            # åˆ›å»ºæ¨¡å‹
            input_size = X_train.shape[1]
            output_size = len(np.unique(y_encoded))
            model = FixedNN(input_size, 128, output_size).cuda()
            
            # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.CrossEntropyLoss()
            
            # è®­ç»ƒ
            start_time = time.time()
            model.train()
            for epoch in range(50):
                optimizer.zero_grad()
                outputs = model(X_train_tensor)
                loss = criterion(outputs, y_train_tensor)
                loss.backward()
                optimizer.step()
                
                if (epoch + 1) % 10 == 0:
                    print(f"  Epoch {epoch+1}/50, Loss: {loss.item():.4f}")
            
            torch.cuda.synchronize()
            nn_time = time.time() - start_time
            
            # è¯„ä¼°
            model.eval()
            with torch.no_grad():
                outputs = model(X_test_tensor)
                _, predicted = torch.max(outputs, 1)
                nn_accuracy = (predicted == y_test_tensor).float().mean().item()
            
            results['pytorch_nn'] = {
                'model': model,
                'time': nn_time,
                'accuracy': nn_accuracy,
                'gpu': True
            }
            
            print(f"PyTorch NN: {nn_time:.2f}ç§’, å‡†ç¡®ç‡: {nn_accuracy:.4f}")
            
        else:
            print("âš ï¸ GPU ä¸å¯ç”¨ï¼Œè·³è¿‡ PyTorch è®­ç»ƒ")
            
    except Exception as e:
        print(f"âš ï¸ PyTorch è®­ç»ƒå¤±è´¥: {e}")
    
    return results

def save_results_simple(results, scaler, label_encoder):
    """ç®€åŒ–ç‰ˆç»“æœä¿å­˜"""
    print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
    
    # åˆ›å»ºæ¨¡å‹ç›®å½•
    os.makedirs('BlockFW/ml/models', exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        'fix_time': datetime.now().isoformat(),
        'results': {
            name: {
                'time': result['time'],
                'accuracy': result['accuracy'],
                'gpu': result['gpu']
            } for name, result in results.items()
        },
        'best_model': max(results.keys(), key=lambda k: results[k]['accuracy']),
        'best_accuracy': max(result['accuracy'] for result in results.values())
    }
    
    report_path = f'BlockFW/ml/models/simple_report_{timestamp}.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # å°è¯•ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    try:
        import joblib
        import torch
        
        for model_name, result in results.items():
            if model_name == 'pytorch_nn':
                model_path = f'BlockFW/ml/models/simple_{model_name}_{timestamp}.pth'
                torch.save(result['model'].state_dict(), model_path)
            else:
                model_path = f'BlockFW/ml/models/simple_{model_name}_{timestamp}.pkl'
                joblib.dump(result['model'], model_path)
            
            print(f"ä¿å­˜ {model_name}: {model_path}")
        
        # ä¿å­˜é¢„å¤„ç†å™¨
        scaler_path = f'BlockFW/ml/models/simple_scaler_{timestamp}.pkl'
        joblib.dump(scaler, scaler_path)
        
        encoder_path = f'BlockFW/ml/models/simple_encoder_{timestamp}.pkl'
        joblib.dump(label_encoder, encoder_path)
        
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        print("ä½†æŠ¥å‘Šå·²æˆåŠŸä¿å­˜")
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–ç‰ˆå¿«é€Ÿä¿®å¤")
    print("="*60)
    
    # 1. ä¿®å¤ GPU è®¡ç®—æµ‹è¯•
    gpu_ok = fix_gpu_computation_test()
    
    # 2. ä¿®å¤æ•°æ®åŠ è½½
    data = fix_data_loading()
    if data is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return
    
    # 3. ä¿®å¤æ•°æ®é¢„å¤„ç†
    X_scaled, y_encoded, scaler, le = fix_data_preprocessing(data)
    
    # 4. è®­ç»ƒä¿®å¤åçš„æ¨¡å‹
    results = train_fixed_models(X_scaled, y_encoded)
    
    if not results:
        print("âŒ æ²¡æœ‰æˆåŠŸè®­ç»ƒä»»ä½•æ¨¡å‹")
        return
    
    # 5. ä¿å­˜ç»“æœ
    report = save_results_simple(results, scaler, le)
    
    # 6. æ˜¾ç¤ºç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š ä¿®å¤ç»“æœæ€»ç»“:")
    print(f"GPU è®¡ç®—æµ‹è¯•: {'âœ“ é€šè¿‡' if gpu_ok else 'âš ï¸ éƒ¨åˆ†é€šè¿‡'}")
    print(f"æ•°æ®åŠ è½½: âœ“ æˆåŠŸ")
    print(f"æ•°æ®é¢„å¤„ç†: âœ“ æˆåŠŸ")
    print(f"æ¨¡å‹è®­ç»ƒ: âœ“ æˆåŠŸ")
    
    print("\nè®­ç»ƒç»“æœ:")
    for name, result in results.items():
        gpu_status = "GPU" if result['gpu'] else "CPU"
        print(f"  {name} ({gpu_status}): {result['time']:.2f}ç§’, å‡†ç¡®ç‡: {result['accuracy']:.4f}")
    
    print(f"\næœ€ä½³æ¨¡å‹: {report['best_model']}")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {report['best_accuracy']:.4f}")
    
    print("\nğŸ‰ ç®€åŒ–ç‰ˆä¿®å¤å®Œæˆï¼")
    print("æŠ¥å‘Šå·²ä¿å­˜åˆ° BlockFW/ml/models/ ç›®å½•")

if __name__ == "__main__":
    main() 